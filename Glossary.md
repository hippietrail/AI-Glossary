## Glossary of AI Jargon and Terminology

- **[Activation Function](https://en.wikipedia.org/wiki/Activation_function)**: A function that determines the output of a neuron in a ***neural network***. It introduces ***non-linearity*** to the network. See ***ReLU***.
- **Adaption Tuning**: _See: ***Fine-Tuning***_
- [**AGI**: **A**rtificial **G**eneral **I**ntelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence). Highly autonomous systems that outperform humans at most economically valuable work. See also ***ASI***.
- **AI slop**: Low-effort, low-quality creative content generated by AI.
- **[AI winter](https://en.wikipedia.org/wiki/AI_winter)**: A period of time when interest and funding for AI R&D is significantly reduced due to failing to meet expectations.
- **[Algorithmic Bias](https://en.wikipedia.org/wiki/Algorithmic_bias)**: The presence of systematic and unfair biases in the outcomes produced by algorithms, typically due to biases present in the training data.
- **[Alignment](https://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/AI_alignment)**: The challenge of ensuring that an AI system's goals and behavior align with human values and intentions.
- **[Andrej Karpathy](https://en.wikipedia.org/wiki/Andrej_Karpathy)**: Key figure in ***CNN***s and computer vision. Director of AI at Tesla.
- **[Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)**: Key figure in ***Machine Learning*** and ***Deep Learning***. Founder of DeepLearning.AI.
- [**ANN**, **A**rtificial **N**eural **N**etwork](https://en.wikipedia.org/wiki/Artificial_neural_network). A computational model inspired by the structure of biological neural networks, consisting of an interconnected network of nodes or neurons.
- **ASI**: **A**rtificial **S**uper**I**ntelligence. ***Artificial intelligence*** where machines surpass human intelligence in virtually every aspect.
- [**Attention**, **Attention Mechanism**](https://en.wikipedia.org/wiki/Attention_(machine_learning)): A mechanism in ***neural networks***, particularly in ***Transformer***-based models, to capture contextual relationships between words in an input sequence.
- **Attention Head**: In the context of neural networks, attention heads refer to the individual components responsible for attending to different parts of the input sequence.
- **[Autodiff, Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)**: Techniques for evaluating the *partial derivative* of a function. Used for implementing ***Backpropagation*** in ***Neural Networks***. See also ***Chain Rule***.
- **[Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)**: A type of neural network architecture used for ***unsupervised learning*** and dimensionality reduction.
- **Autograd**: See ***Autodiff***.
- [**Autoregression**, **Autoregressive**](https://en.wikipedia.org/wiki/Autoregressive_model): A statistical model that predicts the next value based on the previous values.
- **[Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)**: An algorithm used in neural networks to calculate the gradient of the ***loss function*** with respect to the parameters of the network. See ***Geoffrey Hinton***.
- **[Beam Search](https://en.wikipedia.org/wiki/Beam_search)**: A search algorithm used in natural language processing tasks, such as machine translation or text generation, to find the most likely sequence of words given a set of candidate options.
- [**BERT**: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers](https://en.wikipedia.org/wiki/BERT_(language_model)): An architecture based on ***Transformer***s used in ***natural language processing***.
- **Biases**: In an ***artificial neural network***, parameters that add a constant value to the input. See also ***Weights***. Not to be confused with ***Algorithmic Bias***.
- **The Bitter Lesson**: *TBD*
- **[Black Box](https://en.wikipedia.org/wiki/Black_box#Science_and_technology)**: A model that is not easily understood by humans.
- [**BPE**: **B**yte **P**air **E**ncoding](https://en.wikipedia.org/wiki/Byte_pair_encoding). a sub-word tokenization technique used in natural language processing and machine learning. Introduced by Sennrich et al. in their paper "Neural Machine Translation of Rare Words with Subword Units" (2016)
- **[Chain Rule](https://en.wikipedia.org/wiki/Chain_rule)**: A concept in calculus used by ***Autodiff*** for finding the derivatives which is essential in ***Backpropagation***.
- **[ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)**: The first ***LLM*** that is capable of generating human-like text.
- **Classifier**: A ***machine learning*** model that can be used to classify data.
- [**CNN**: **C**onvolutional **N**eural **N**etwork](https://en.wikipedia.org/wiki/Convolutional_neural_network): A type of neural network architecture commonly used for image and video processing.
- **Context Window**: A window of input data that is used to predict the next word.
- [**CoT**: **C**hain **O**f **T**houghts](https://en.wikipedia.org/wiki/Prompt_engineering#Chain-of-thought): *TBD*.
- **Deep Contrastive Network**: *TBD*
- **[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning)**: A subfield of ***machine learning*** that focuses on the development and application of ***artificial neural networks*** with multiple layers.
- [**Diffusion**, **Diffusion Model**](https://en.wikipedia.org/wiki/Diffusion_model): A technique used in ***Generative AI*** which involves learning how to remove blur/noise from images.
- **Domain Adaption**: _See: ***Fine-Tuning***_
- **Doomer**, **Doomerism**: A disparaging term used by AI optimists to describe AI pessimists.
- **[Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky)**: A prominent figure in ***Alignment***
- **[ELIZA](https://en.wikipedia.org/wiki/ELIZA)**: A very early and simplistic ***natural language processing*** chatbot from the 1960s that nonetheless fooled many people into thinking it was human.
- **Embedding**: See also ***Latent Space***, ***Latent Variable***, ***Word Embedding***
- **[Feedforward Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network)**: A type of ***artificial neural network*** where information flows from the input layer, through any hidden layers, to the output layer with no feedback.
- **[Fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))**: The process of further training a pre-trained model on a specific task or dataset to improve its performance by updating the its parameters based on the new data while retaining the knowledge learned during pre-training. Also known as: ***Adaption Tuning***, ***Domain Adaption***
- **FOOM**: *TBD*. See also ***Hard Takeoff***, ***Elizezer Yudkowsky***.
- **Foundation Model**: The category of which the ***LLMs*** are the most well known member. They are not limited to text, but cover all modalities and work by segmenting data into ***Tokens*** or ***Patches***.
- [**GAN**: **G**enerative **A**dversarial **N**etwork](https://en.wikipedia.org/wiki/Generative_adversarial_network): A type of ***machine learning*** model involving a ***generator*** AI and a ***discriminator*** AI. The former tries to generate realistic output the latter can't detect while the latter tries to detect whether input is real or generated.
- **[Generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)**: Algorithms or models that can create new content, including text, audio, or images.
- **[Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton)**: Most known for his working developing ***Backpropagation***, a key breakthrough in ***deep learning***.
- **Glitch Token**: A type of token in an ***LLM*** that can cause anomalous, unexpected, or nonsensical output apparently unrelated to the prompt. For example "SolidGoldMagikarp".
- [**GPT**: **G**enerative **P**re-trained **T**ransformer](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer). A type of ***LLM*** that utilizes the ***Transformer*** architecture and is trained on a large corpus of text data. GPT models have been successful in various natural language processing tasks, including text generation, language translation, and question-answering.
- **[Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)**: An optimization algorithm used in ***machine learning*** to minimize the ***loss function*** of a model.
- **Guardrails**: A nontechnical umbrella term for various safety measures that attempt to counter toxicity, bias, etc.
- **[Hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))**: A nontechnical term for generated LLM output that is not based on fact.
- **Hard Takeoff**: A scenario in which ***AGI*** rapidly surpasses human intelligence, potentially leading to an uncontrollable impact on society.
- **Hidden Layer**: A layer in a neural network that is not visible to the user.
- **[Hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))**: Parameter that is not directly related to the model architecture.
- **[Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever)**: ***Transformers***. Co-founder of ***OpenAI***.
- [**In-Context Learning**, **ICL**](https://en.wikipedia.org/wiki/Prompt_engineering#In-context_learning): The ability of ***Large Language Models*** to learn from information in the input context without needing to update model parameters by updating the state of latent variables based on the context, and conditioning on this when predicting the next output.
- **Inference**: The process of using a model to make predictions on unseen data.
- **Inference Kernel**: *TBD*
- **Inner Alignment**: *TBD*
- **Instruction Tuning**: *TBD*
- **Interpretability**: The degree to which we can understand the output of an ***LLM***.
- **Jailbreak, Jailbreaking**: Circumventing the ***Guardrails*** of an ***LLM*** with cleverly designed ***prompts***.
- **[Language Model](https://en.wikipedia.org/wiki/Language_model)**: A computational model of human language built from statistical data.
- **[Latent Space](https://en.wikipedia.org/wiki/Latent_space)**: *TBD*. See also **Embedding**.
- **[Latent Variable](https://en.wikipedia.org/wiki/Latent_variable)**: *TBD*. See also **Embedding**.
- **[Layer](https://en.wikipedia.org/wiki/Layer_(deep_learning))**: A set of artificial neurons that are not connected to each other but take input from the previous layer and pass their output to the next layer. Each layer may be seen as a level of generalization or abstraction.
- [**LLM**: **L**arge **L**anguage **M**odel](https://en.wikipedia.org/wiki/Large_language_model). A type of ***Language Model*** that uses the ***Transformer*** architecture and is trained on a large corpus of text data. The most well-known category of ***Foundation Model***.
- **Logit**: *TBD*
- **[Loss Function](https://en.wikipedia.org/wiki/Loss_function)**: In training a neural network, a function that measures how far the network's output is from the desired output.
- [**LSTM**: **L**ong **S**hort-**T**erm **M**emory](https://en.wikipedia.org/wiki/Long_short-term_memory). A type of ***neural network*** architecture that is commonly used for sequence data processing.
- **[Machine Learning](https://en.wikipedia.org/wiki/Machine_learning)**: *TBD*
- **[Machine Translation](https://en.wikipedia.org/wiki/Machine_translation)**: Methods to translate text from one human language to another that may include ***NLP***, statistics and probability, or more advanced ***Deep learning*** techniques such as ***LLM***s.
- **Maximum Likelihood Estimation**: *TBD*
- **Memorization**: See also ***Overfitting***. *TBD*
- [**MoE**: **M**ixture **o**f **E**xperts](https://en.wikipedia.org/wiki/Mixture_of_experts)**: *TBD*
- **Modality**: The type of data which a model uses, such as text, images, audio, and video. See also ***Multimodal***, ***Multimodality***.
- **Model Collapse**: *TBD*.
- **Multi-Head Attention**: *TBD*. See also ***Attention***, ***Attention Head***.
- [**Multimodal**, **Multimodality**](https://en.wikipedia.org/wiki/Multimodal_learning): Refers to AI technologies that can be trained on and make inferences on multiple kinds of data, such as images, audio, and video in addtion to text.
- **[Neural Network](https://en.wikipedia.org/wiki/Neural_network)**: *TBD*
- [**NLP**: **N**atural **L**anguage **P**rocessing](https://en.wikipedia.org/wiki/Natural_language_processing). *TBD*
- **One Shot Learning**: *TBD*
- **OOD**, **O**ut-**o**f-Distribution: *TBD*
- **OOCR**, **O**ut-**o**f-**C**ontext **R**easoning: *TBD*
- **[OpenAI](https://en.wikipedia.org/wiki/OpenAI)**: AI company prominent for ***GPT***s.
- **Outer Alignment**: *TBD*
- **[Overfitting](https://en.wikipedia.org/wiki/Overfitting)**: *TBD*. Less technically also referred to as *memorization*. It is a situation in which the model learns the training data too well and fails to generalize to new data.
- **Parameter**: *TBD*
- **Patch**: The equivalent of the **Tokens** of an ***LLM*** for other ***Modalities*** of ***Foundation Model*** such as Audio, Speech, Video, etc.
- **P(doom)**: The prior probability of AI causing an existential crisis for humanity.
- **Perceptron**: *TBD*
- [**PoE**: **P**roduct **o**f **E**xperts](https://en.wikipedia.org/wiki/Product_of_experts): *TBD*
- **Positive transfer**: *TBD*
- **Pre-training**: A stage in which a model is trained on a large corpus of text data before being fine-tuned on a specific task or dataset.
- **Prompt**: The text given to an ***LLM*** in the form of a question or command that the model will generate a response to.
- [**Prompt Engineer**, **Prompt Engineering**](https://en.wikipedia.org/wiki/Prompt_engineering): A person/the process of coming up with effective prompts for an ***LLM***.
- **Prompt Injection**: *TBD*
- **[Q-learning](https://en.wikipedia.org/wiki/Q-learning)**: *TBD*
- **[Q\*](https://en.wikipedia.org/wiki/Q*)**: *TBD*
- [**Reinforcement Learning**, **RL**](https://en.wikipedia.org/wiki/Reinforcement_learning): *TBD*
- **[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))**: A type of ***activation function*** that is used in neural networks to introduce non-linearity.
- **Retrieval-Augmented Generation**, **RAG**: *TBD*
- **Reward Function**: *TBD*
- **Reward Hacking**: *TBD*
- [**RLHF**: **R**einforcement **L**earning from **H**uman **F**eedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback): One of the ***Guardrails*** that is part of the ***Fine-Tuning*** process that attempts to align a trained model to human values and preferences. See **Alignment**.
- [**RNN**: **R**ecurrent **N**eural **N**etwork](https://en.wikipedia.org/wiki/Recurrent_neural_network): A type of neural network architecture commonly used for sequential data processing such as audio and text.
- **Safeguards**: _See: **Guardrails**_
- **[Scaling](https://en.wikipedia.org/wiki/Neural_scaling_law)**: *TBD*
- **Scaling Hypothesis**: *TBD*
- **Self-Attention**: _See: **Attention, Attention Mechanism**_
- [**SGD, Stochastic Gradient Descent**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): *TBD*
- **[The Singularity](https://en.wikipedia.org/wiki/Technological_singularity)**: The posited point in the future when AI will surpass human intelligence.
- **Softmax**: *TBD*
- **[Sparse Autoencoder](https://en.wikipedia.org/wiki/Autoencoder#Sparse_autoencoder_(SAE))**: A type of ***Autoencoder*** inspired by the Sparse Coding Hypothesis in neuroscience, in which only a small number of neurons are activated at a time.
- **State space model** (**SSM**): *TBD*
- **[Stochastic Parrot](https://en.wikipedia.org/wiki/Stochastic_parrot)**: Coined by Emily M. Bender in 2021. Disparaging term used of LLMs to refute that they may have any inherent world-building.
- **[Style Transfer](https://en.wikipedia.org/wiki/Neural_style_transfer)**: In image generation, a technique where a style image in used to modify an input image.
- **[Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)**: *TBD*
- **[Synthetic Data](https://en.wikipedia.org/wiki/Synthetic_data)**: Algorithmically generated data used for training and validating models.
- **System Prompt**: *TBD*
- **[Temperature](https://learnprompting.org/docs/basics/configuration_hyperparameters#temperature)**: *TBD*
- **Token**: A unit of information in an ***LLM*** that roughly corresponds to a word in the vocabulary but is very often only part of a word. See also **Patch**.
- **[Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning)**: *TBD*
- **[Transformer](https://en.wikipedia.org/wiki/Transformer_(machine-learning_model))**: A ***neural network*** architecture introduced in the paper "Attention is All You Need" by Vaswani et al. (2017). It has become a popular model for various natural language processing tasks. The Transformer architecture utilizes self-attention mechanisms to capture contextual relationships between words in an input sequence, enabling effective modeling of long-range dependencies.
- **[Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)**: *TBD*
- **[Weights](https://en.wikipedia.org/wiki/Weight_(disambiguation)#Science_and_technology)**: *TBD*. See also ***Biases***
- **[Word Embedding](https://en.wikipedia.org/wiki/Word_embedding)**: A representation of a word or token as a set of numbers in a vector space.
- **World Model**: Implicit representation of the world encoded in the ***weights*** and ***biases*** of a ***neural network*** captured through patterns in the data during training.
- **[Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun)**: ***Deep Learning*** and ***CNN***s. Chief AI Scientist at Facebook.
- **[Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio)**: ***Deep Learning*** and ***Neural Networks***.
- **[Zero-Shot Learning](https://en.wikipedia.org/wiki/Zero-shot_learning)**: A machine learning paradigm where a model is trained to recognize and classify objects or concepts that it has never seen before. It uses auxiliary information about the unseen classes to generalize from the known classes.
