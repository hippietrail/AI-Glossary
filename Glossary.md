## Glossary of AI Jargon and Terminology

- **Activation Function**: *TBD*
- **AGI**: *TBD*
- **Alignment**: The challenge of ensuring that an AI system's goals and behavior align with human values and intentions.
- **Andrej Karpathy**: ***CNN***s. Director of AI at Tesla.
- **Andrew Ng**: ***Machine Learning*** and ***Deep Learning***.
- **ASI**: *TBD*
- **Attention**: A mechanism in ***neural networks***, particularly in ***Transformer***-based models, to capture contextual relationships between words in an input sequence.
- **Attention Head**: In the context of neural networks, attention heads refer to the individual components responsible for attending to different parts of the input sequence.
- **Backpropagation**: *TBD*
- **Beam Search**: A search algorithm used in natural language processing tasks, such as machine translation or text generation, to find the most likely sequence of words given a set of candidate options.
- **BERT**: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers: *TBD*
- **Biases**: *TBD*. See also ***Weights**
- **BPE**: **B**yte **P**air **E**ncoding. a sub-word tokenization technique used in natural language processing and machine learning. Introduced by Sennrich et al. in their paper "Neural Machine Translation of Rare Words with Subword Units" (2016)
- **ChatGPT**: *TBD*
- **CNN**: **C**onvolutional **N**eural **N**etwork, a type of neural network architecture commonly used for image and video processing.
- **Deep Learning**: *TBD*
- **Eliezer Yudkowsky**: ***Alignment***
- **Fine-tuning**: The process of further training a pre-trained model on a specific task or dataset to improve its performance by updating the its parameters based on the new data while retaining the knowledge learned during pre-training.
- **GAN**: **G**enerative **A**dversarial **N**etwork: *TBD*
- **Generative AI**: Algorithms or models that can create new content, including text, audio, or images.
- **Geoffrey Hinton**: ***Backpropagation***.
- **Glitch Token**: A type of token in an ***LLM*** that can cause anomalous, unexpected, or nonsensical output apparently unrelated to the prompt. For example "SolidGoldMagikarp".
- **GPT**: **G**enerative **P**re-trained **T**ransformer". A type of ***LLM*** that utilizes the ***Transformer*** architecture and is trained on a large corpus of text data. GPT models have been successful in various natural language processing tasks, including text generation, language translation, and question-answering.
- **Guardrails**: A nontechnical umbrella term for various safety measures or constraints put in place to prevent undesirable outcomes or behaviors in AI systems.
- **Hidden Layer**: *TBD*
- **Hyperparameter**: *TBD*
- **Ilya Sutskever**: ***Transformers***. Co-founder of ***OpenAI***.
- **Inference**: *TBD**
- **LLM**: **L**arge **L**anguage **M**odel.
- **Loss Function**: *TBD*
- **Machine Learning**: *TBD*
- **Neural Network**: *TBD*
- **OpenAI**: *TBD*
- **Overfitting**: *TBD*. Less technically also referred to as *memorization*.
- **Pre-training**: *TBD*
- **Q-learning**: *TBD*
- **Reinforcement Learning**: *TBD*
- **RLHF**: **R**einforcement **L**earning from **H**uman **F**eedback
- **RNN**: **R**ecurrent **N**eural **N**etwork, a type of neural network architecture commonly used for sequential data processing such as audio and text.
- **Stochastic Parrot**: *TBD*. Coined by Emily M. Bender in 2021.
- **Transfer Learning**: *TBD*
- **Transformer**: A ***neural network*** architecture introduced in the paper "Attention is All You Need" by Vaswani et al. (2017). It has become a popular model for various natural language processing tasks. The Transformer architecture utilizes self-attention mechanisms to capture contextual relationships between words in an input sequence, enabling effective modeling of long-range dependencies.
- **Weights**: *TBD*. See also ***Biases***
- **Yann LeCun**: ***Deep Learning*** and ***CNN***s. Chief AI Scientist at Facebook.
- **Yoshua Bengio**: ***Deep Learning*** and ***Neural Networks***.
